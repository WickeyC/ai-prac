{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "### NLP\n",
    "\n",
    "▪ NLP is a machine learning technology that gives computers the ability to interpret, manipulate, and comprehend human language.\n",
    "\n",
    "<img src=\"nlp.jpg\" width=\"500\">\n",
    "\n",
    "### Structured Data vs. Unstructured Data\n",
    "\n",
    "▪ Structured data is standardized, clearly defined, and searchable data, while unstructured data is raw data of various forms.\n",
    "\n",
    "<img src=\"structured_vs_unstructured_data.png\" width=\"500\">\n",
    "\n",
    "### Garbage In, Garbage Out\n",
    "\n",
    "▪ Garbage in, garbage out is a concept common to computer science where the quality of output is determined by the quality of the input. \n",
    "\n",
    "<img src=\"garbage.png\" width=\"500\">\n",
    "\n",
    "### Text Preprocessing\n",
    "\n",
    "<img src=\"text_preprocessing.png\" width=\"700\">\n",
    "\n",
    "▪ Unstructured data must first be cleaned and pre-processed before analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35ABic3Ak02z"
   },
   "source": [
    "# NLTK and Preprocessing Techniques\n",
    "\n",
    "### NLP Toolkits\n",
    "\n",
    "▪ NLTK, which stands for Natural Language Toolkits, is a suite of libraries built for working with NLP in Python.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "▪ NLTK and NLTK dataset\n",
    "\n",
    "### Installing NLTK via Anaconda Prompt\n",
    "\n",
    "pip install nltk\n",
    "\n",
    "### Installing NLTK Dataset via Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# The following command downloads all data and models, and it will take awhile\n",
    "# Do this step only if nltk_data is not available on your pc\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLCozzHjk022"
   },
   "source": [
    "## Text Data\n",
    "\n",
    "▪ Text data is messy.\n",
    "\n",
    "▪ To analyze this data, it has to be preprocessed into clean text in a format that machine models can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvhK13m5k025"
   },
   "source": [
    "![](https://i.imgur.com/3L6x92C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data: Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"Hi Mr. Smith! I'm going to buy some vegetables \\\n",
    "(2 tomatoes and 4 cucumbers) from the store. Should I pick up some black-eyed peas as well?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLRudDeLk026"
   },
   "source": [
    "## Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMlRIFT8k026"
   },
   "outputs": [],
   "source": [
    "import re # Regular expression library\n",
    "\n",
    "clean_text = re.sub('[%s]' %(string.punctuation), '', original_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pynative.com/python-regex-special-sequences-and-character-classes/\n",
    "\n",
    "clean_text = re.sub('[^\\w\\s]','', original_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoaBcONOk027"
   },
   "source": [
    "## Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5e8Ot44k027"
   },
   "outputs": [],
   "source": [
    "# Removes all words containing digits\n",
    "clean_text = re.sub('\\d', '', clean_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = re.sub('[^A-Za-z\\s]','', original_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFSU428Ck026"
   },
   "source": [
    "## Covert Text to Lowercase\n",
    "\n",
    "<img src=\"vegetarian.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"The Nature's Vegetarian Restaurant located at Bangsar is a fantastic vegetarian restaurant.\"\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6hcxk1ak027"
   },
   "outputs": [],
   "source": [
    "clean_text = clean_text.lower()\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jBmgXb7k023"
   },
   "source": [
    "## Word Tokenization (original_text)\n",
    "\n",
    "▪ Tokenization is the process of breaking down a phrase, sentence, paragraph, or an entire text document into smaller units.\n",
    "\n",
    "<img src=\"tokenization.jpeg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"Hi Mr. Smith! I'm going to buy some vegetables \\\n",
    "(2 tomatoes and 4 cucumbers) from the store. Should I pick up some black-eyed peas as well?\"\n",
    "\n",
    "print(original_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#1 Word Tokenization with word_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYjeWTf0k023"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(original_text) \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#2 Word Tokenization with regexp_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize\n",
    "\n",
    "tokens = regexp_tokenize(original_text, pattern = '\\w+')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#3 Word Tokenization with split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = original_text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#4 Word Tokenization with Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.split(\"\\W+\", original_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.findall(\"\\w+\", original_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization (clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#1 Word Tokenization with word_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(clean_text) \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#2 Word Tokenization with regexp_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize\n",
    "\n",
    "tokens = regexp_tokenize(clean_text, pattern = '\\w+')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#3 Word Tokenization with split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = clean_text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#4 Word Tokenization with Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.split(\"\\W+\", clean_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.findall(\"\\w+\", clean_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzZ_SG_kk024"
   },
   "source": [
    "## Sentence Tokenization (original_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#1 Sentence Tokenization with sent_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wi834t7Ek024",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokens = sent_tokenize(original_text)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#2 Sentence Tokenization with split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = original_text.split(\". \")\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = re.split(\"[?!.] \", original_text)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGP8X7cak027"
   },
   "source": [
    "## Remove Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2SzAuxbk028"
   },
   "source": [
    "![](https://i.imgur.com/T5RJXrX.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print English Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjdZhGrMk028"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Stop Words from the following Sentence\n",
    "\n",
    "<h3><center>Stopwords are a commonly used words that generally don’t contribute anything to the meaning of the text.</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the list of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.sort() # sorted()\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Language-X Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_2 = stopwords.words('danish')\n",
    "print(stop_words_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBTaQAS-k028"
   },
   "source": [
    "### Remove Stop Words from clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_x_stopwords = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "print(tokens_x_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKr2I0-Jk029"
   },
   "source": [
    "## Stemming\n",
    "\n",
    "![](https://i.imgur.com/9qllh8j.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming with LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_1 = ['Connects', 'Connecting', 'Connections', 'Connected', 'Connection', 'Connectings', 'Connect']\n",
    "words_2 = ['drive', 'drives', 'driver', 'drivers', 'driven', 'driving']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc_stemmer = LancasterStemmer()\n",
    "\n",
    "for word_1 in words_1:\n",
    "    print(word_1, \"-->\", lc_stemmer.stem(word_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-okFUv-k02-",
    "outputId": "d3f6ffe6-8c23-4adf-f9f6-e52e71e16045"
   },
   "outputs": [],
   "source": [
    "for word_2 in words_2:\n",
    "    print(word_2, \"-->\", lc_stemmer.stem(word_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming with PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pt_stemmer = PorterStemmer()\n",
    "\n",
    "for word_1 in words_1:\n",
    "    print(word_1, \"-->\", pt_stemmer.stem(word_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_2 in words_2:\n",
    "    print(word_2, \"--->\", pt_stemmer.stem(word_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming with SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sb_stemmer = SnowballStemmer(language = 'english')\n",
    "\n",
    "for word_1 in words_1:\n",
    "    print(word_1, \"--->\", sb_stemmer.stem(word_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_2 in words_2:\n",
    "    print(word_2, \"--->\", sb_stemmer.stem(word_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeXl-dNTk02_",
    "outputId": "e2964c96-a7b0-40c7-a25c-cd4361cd29a0"
   },
   "source": [
    "## Lemmatization\n",
    "\n",
    "![](https://i.imgur.com/9qllh8j.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "wn_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word_1 in words_1:\n",
    "    print(word_1, \"--->\", wn_lemmatizer.lemmatize(word_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_2 in words_2:\n",
    "    print(word_2, \"--->\", wn_lemmatizer.lemmatize(word_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with WordNetLemmatizer on clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_x_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_x_stopwords = [wn_lemmatizer.lemmatize(word_1) for word_1 in tokens_x_stopwords]\n",
    "\n",
    "print(lemma_x_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icdQoHrrk03A"
   },
   "source": [
    "## Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgyUyDZfk02_"
   },
   "source": [
    "![](https://i.imgur.com/8edVsCR.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.help.upenn_tagset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging on Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiFKoaQ2k03A"
   },
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "text_1 = \"James Smith lives in the United States.\"\n",
    "\n",
    "tokens = pos_tag(word_tokenize(text_1))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging on original_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pos_tag(word_tokenize(original_text))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging on clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pos_tag(lemma_x_stopwords)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRjifONik03A"
   },
   "source": [
    "## Chunking\n",
    "\n",
    "▪ Chunking is a step following POS tagging and structuring the sentence in \"chunks\" by identifying continuous words that can be grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6xqPDx0k03A"
   },
   "outputs": [],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "text_1 = \"James Smith lives in the United States.\"\n",
    "tokens = pos_tag(word_tokenize(text_1))\n",
    "\n",
    "# This extracts entities from the list of words\n",
    "result = ne_chunk(tokens) \n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = \"The Nature's Vegetarian Restaurant located at Bangsar is a fantastic vegetarian restaurant.\"\n",
    "tokens = pos_tag(word_tokenize(text_2))\n",
    "\n",
    "result = ne_chunk(tokens) \n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3mdUZ4Mk03A"
   },
   "source": [
    "## Compound Term Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1Qw0PuWk03B"
   },
   "source": [
    "![](https://i.imgur.com/q1WuWai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH4qmhFVk03B"
   },
   "source": [
    "### Compound Term Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer \n",
    "\n",
    "mwe_tokenizer = MWETokenizer([('James', 'Smith'), ('United', 'States')])\n",
    "\n",
    "text_1 = \"James Smith lives in the United States.\"\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text_1))\n",
    "mwe_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokenizer = MWETokenizer([(\"Nature's\", \"Vegetarian\", \"Restaurant\"), ('Subang', 'Jaya')])\n",
    "\n",
    "text_3 = \"The Nature's Vegetarian Restaurant located at Subang Jaya is a fantastic vegetarian restaurant.\"\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text_3))\n",
    "mwe_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Numpy Functionality\n",
    "\n",
    "▪ NumPy, which stands for Numerical Python, is a Python library used for working with arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an ndarray (2D Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr_1d = np.array([2, 4, 6, 8]) \n",
    "arr_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(arr_1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_2d = np.random.randn(6, 4)\n",
    "arr_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oqFOp9nk03B"
   },
   "source": [
    "## Basic Pandas Functionality\n",
    "\n",
    "▪ Pandas stands for \"Python Data Analysis Library\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot21kY7mk03B"
   },
   "source": [
    "![](https://i.imgur.com/HpgLFOT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe\n",
    "\n",
    "▪ A dataframe is a 2-dimensional labeled data structure with columns of potentially different types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(arr_2d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Labels of Rows and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Labels to Rows and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['A', 'B', 'C', 'D']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = ['Rec_1', 'Rec_2', 'Rec_3', 'Rec_4', 'Rec_5', 'Rec_6']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cookie_reviews.csv\n",
    "\n",
    "### Reading data from cookie_reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cookie_reviews.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the Labels of All Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print All Values of a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Slicing\n",
    "\n",
    "▪ Python iloc() function is used for integer-location based indexing / selection by position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYG95Kw0k03C"
   },
   "outputs": [],
   "source": [
    "df.iloc[0] # first row of data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-1] # last row of data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,0] # first column of data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,-1] # last column of data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0, 1] # first row, second column of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0:4, 0:2] # first 4 rows and first 2 columns of data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73NlJvM_k03D"
   },
   "source": [
    "## Preprocessing Exercise: cookie_reviews.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhpoyqpfk03E"
   },
   "source": [
    "#### Question 1: Determine how many reviews there are in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9Dz0atok03E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l63g7P8fk03E"
   },
   "source": [
    "#### Question 2: Determine the percentage of 1, 2, 3, 4 and 5 star reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BKYT60ck03E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9frnQ720k03E"
   },
   "source": [
    "#### Question 3: Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqY6OLimk03E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uubKAy_ak03F"
   },
   "source": [
    "#### Question 4: Change to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be2IxAEfk03F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mTwPsiFk03F"
   },
   "source": [
    "#### Question 5: Perform stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjFvaoJzk03F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv9e74jDk03F"
   },
   "source": [
    "## Text Similarity Measures\n",
    "\n",
    "▪ To measure distance between 2 strings.\n",
    "\n",
    "<img src=\"similarity.png\" width=\"700\">\n",
    "\n",
    "▪ Some examples of its application include information retrieval, text classification, document clustering, and topic modeling.\n",
    "\n",
    "### Levenshtein distance\n",
    "\n",
    "▪ **Levenshtein distance** is one way to measure the word similarity. \n",
    "\n",
    "▪ Minimum number of operations to get from one word to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaG7KOEik03F"
   },
   "source": [
    "![](https://i.imgur.com/FkdJmPi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1WK5fP5k03G"
   },
   "source": [
    "# TextBlob\n",
    "\n",
    "▪ Other than NLTK, TextBlob is another Python library for processing textual data.\n",
    "\n",
    "▪ TextBlob capabilities: Tokenization, Parts of speech tagging, Sentiment analysis, Spell check, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUlBT04yk03G"
   },
   "source": [
    "## TextBlob Demo: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1571,
     "status": "error",
     "timestamp": 1625705228446,
     "user": {
      "displayName": "CHING PANG GOH",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhAtxw1eyRJoJoQwsqDiWRYXwwRLKTenuEJ5Ge2vQ=s64",
      "userId": "03150219032412111071"
     },
     "user_tz": -480
    },
    "id": "lYml12fGk03G",
    "outputId": "d7a4566b-87a1-49a9-9ea6-3baa8094f151"
   },
   "outputs": [],
   "source": [
    "#pip install textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "my_text = TextBlob(\"We're moving from NLTK to TextBlob. How fun!\")\n",
    "my_text.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFMRGAVck03G"
   },
   "source": [
    "## TextBlob Demo: Spell Check\n",
    "\n",
    "▪ The correct() function calculates the Levenshtein distance between the word \"graat\" and all words in its word list of the words with the smallest Levenshtein distance, it outputs the most popular word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erc2XHNJk03G",
    "outputId": "4d6a0d9b-bf42-4845-b44d-ddf5c133d05b"
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(\"I'm graat at speling.\")\n",
    "print(blob.correct()) # print function requires Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGGHwF-_k03H"
   },
   "source": [
    "## TextBlob Demo: Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBPNmxdLk03H",
    "outputId": "83ac4941-a17d-4fea-e0af-1d64863b88a3"
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(\"John hits the ball.\")\n",
    "for words, tag in blob.tags:\n",
    "    print (words, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IjrZlWlk03H"
   },
   "source": [
    "## TextBlob Demo: Language Translation\n",
    "\n",
    "▪ Textblob uses Google Translate as its translation engine\n",
    "\n",
    "https://thinkinfi.com/natural-language-processing-using-textblob/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0Jk18Vjk03I",
    "outputId": "7a57f130-4a65-45d9-c7b5-6f0b0b40324e"
   },
   "outputs": [],
   "source": [
    "word = TextBlob(\"Bonjour, comment allez-vous\")\n",
    "word.translate(from_lang = 'fr', to = 'cn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word.translate(from_lang = 'fr', to = 'zh-CN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9P_IjW4ok03I"
   },
   "source": [
    "## Text Format for Analysis: Count Vectorizer\n",
    "\n",
    "![](https://i.imgur.com/OQDeQlb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vt8xtmfCk028"
   },
   "source": [
    "### Features Extraction and CountVectorizer\n",
    "\n",
    "▪ Feature extraction is the process of transforming textual data into numerical data.\n",
    "\n",
    "▪ CountVectorizer is a tool used to vectorize text data by converting it into a matrix of token counts.\n",
    "\n",
    "![](Count-Vectorization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Create a DataFrame from original_text using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = [\"Hi Mr. Smith! I'm going to buy some vegetables \\\n",
    "(2 tomatoes and 4 cucumbers) from the store. Should I pick up some black-eyed peas as well?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lBhjqJGk029"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "           \n",
    "# Incorporate stop words when creating the count vectorizer\n",
    "cv = CountVectorizer(stop_words = 'english') \n",
    "\n",
    "X = cv.fit_transform(original_text)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X.toarray())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X.toarray(), columns = cv.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Create a DataFrame from clean_text using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemma_x_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\" \".join(lemma_x_stopwords)]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer() \n",
    "\n",
    "X = cv.fit_transform(text)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns = cv.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Create a DataFrame from the following Corpus using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71HqDC9fk03I",
    "outputId": "f788a730-e2c0-445a-94da-645b67ce85a2"
   },
   "outputs": [],
   "source": [
    "corpus = ['This is the first document.', \n",
    "          'This is the second document.', \n",
    "          'And the third one. One is fun.']\n",
    "\n",
    "cv = CountVectorizer() \n",
    "\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns = cv.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Create a DataFrame from the following Corpus using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iq-Cszhdk03J",
    "outputId": "bc2dd627-1839-4101-d733-a9842de3e019"
   },
   "outputs": [],
   "source": [
    "corpus = ['The weather is hot under the sun',\n",
    "          'I make my hot chocolate with milk',\n",
    "          'One hot encoding',\n",
    "          'I will have a chai latte with milk',\n",
    "          'There is a hot sale today']\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english') \n",
    "\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "df = pd.DataFrame(X, columns = cv.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GefIA4Jk03J"
   },
   "source": [
    "## Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSF6LaN3k03J"
   },
   "source": [
    "![](https://i.imgur.com/PyirXsy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reTZir8kk03K"
   },
   "source": [
    "### Measuring Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZbnjB3Hk03K",
    "outputId": "c8186453-7535-4086-9499-e394d20da6fb"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "pairs = list(combinations(['A', 'B', 'C', 'D'], 2))\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range() returns an immutable sequence of numbers that can be easily converted to lists\n",
    "x = list(range(5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLI6INUvk03K",
    "outputId": "7df91d90-1c96-4ab5-bc71-143d623be916"
   },
   "outputs": [],
   "source": [
    "# calculate the cosine similarity between all combinations of documents\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# list all combinations of the 5 sentences in pairs, in terms of indexes\n",
    "# (0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), ..., (3,4)\n",
    "pairs = list(combinations(range(len(corpus)), 2)) \n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = [(corpus[a_index], corpus[b_index]) for (a_index, b_index) in pairs]\n",
    "combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
    "results = [cosine_similarity([X[a_index]], [X[b_index]]) for (a_index, b_index) in pairs]\n",
    "sorted(zip(results, combos), reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Which Two Documents are Most Similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63h3cPxxk03N"
   },
   "source": [
    "![](https://i.imgur.com/jrfN6Jj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdYuTyMkk03N"
   },
   "source": [
    "![](https://i.imgur.com/BI8XP92.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysNlUOUck03N"
   },
   "source": [
    "![](https://i.imgur.com/3IbfQXT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaYfQGbok03N"
   },
   "source": [
    "![](https://i.imgur.com/pnNqzql.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer vs. TfidfVectorizer\n",
    "\n",
    "▪ Original documents\n",
    "\n",
    "![](table2.png)\n",
    "\n",
    "▪ Documents with stopwords removed\n",
    "\n",
    "![](table1.png)\n",
    "\n",
    "▪ Feature extraction with CountVectorizer\n",
    "\n",
    "![](table3.png)\n",
    "\n",
    "Feature extraction with TfidfVectorizer\n",
    "\n",
    "![](table4.png)\n",
    "\n",
    "https://medium.com/codex/document-indexing-using-tf-idf-189afd04a9fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOxJucLNk03N",
    "outputId": "bbf2c1d1-89ec-4a6a-fa3a-2ab044b7e427"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['This is the first document.',\n",
    "          'This is the second document.',\n",
    "          'And the third one. One is fun.']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "df = pd.DataFrame(X, columns=cv.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwU3Q63Bk03O",
    "outputId": "5f405d59-5a48-4988-89c3-1cc8dcba43f9"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cv_tfidf = TfidfVectorizer()\n",
    "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
    "df_tfidf = pd.DataFrame(X_tfidf, columns=cv_tfidf.get_feature_names_out())\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYpkjzzik03O"
   },
   "source": [
    "![](https://i.imgur.com/xlJibKw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0dZnaIzk03O"
   },
   "source": [
    "### Document Similarity: Example with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Jwp0N7ok03O",
    "outputId": "66ab19c6-dd92-415e-b32c-63d8e477ff8e"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['The weather is hot under the sun',\n",
    "          'I make my hot chocolate with milk',\n",
    "          'One hot encoding',\n",
    "          'I will have a chai latte with milk',\n",
    "          'There is a hot sale today']\n",
    "\n",
    "# Create the document-term matrix with TF-IDF vectorizer\n",
    "cv_tfidf = TfidfVectorizer(stop_words = \"english\")\n",
    "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
    "dt_tfidf = pd.DataFrame(X_tfidf, columns = cv_tfidf.get_feature_names_out())\n",
    "dt_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0K4-aRyIk03P",
    "outputId": "98b5cf38-511b-4b8d-bb11-973afa5aa989"
   },
   "outputs": [],
   "source": [
    "# calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
    "results_tfidf = [cosine_similarity([X_tfidf[a_index]], [X_tfidf[b_index]]) for (a_index, b_index) in pairs]\n",
    "sorted(zip(results_tfidf, combos), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Which Two Documents are Most Similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abfHLHtzk03P"
   },
   "source": [
    "![](https://i.imgur.com/mj4J60v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_344udsk03P"
   },
   "source": [
    "## Text Similarity Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04ubfzrMk03P"
   },
   "source": [
    "We will be using a song lyric dataset from Kaggle to identify songs with similar lyrics. The data set contains artists, songs and lyrics for 55K+ songs, but today we will be focusing on songs by one group in particular - The Beatles. The following code will help you load in the data and get set up for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwTzQeppk03Q"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5F7qQISVk03Q"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('songdata.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fx7fvF-mk03Q"
   },
   "source": [
    "### Question 1: Note the '\\n' (new line) characters in the lyrics. Remove them using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KL66xQSJk03Q"
   },
   "outputs": [],
   "source": [
    "# Code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6UNCrJUk03Q"
   },
   "source": [
    "### Question 2: List all the rows with \"Imagine\" in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRZO0iWyk03Q"
   },
   "outputs": [],
   "source": [
    "# Code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JYSXoK5k03R"
   },
   "source": [
    "### Question 3: Extract the first line of lyric out from the first song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Trrywo8Xk03R"
   },
   "outputs": [],
   "source": [
    "# Code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5QT9hM8k03R"
   },
   "source": [
    "### Question 4: Find out the sentiment of the extracted lyric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1m6qTGrPk03R"
   },
   "outputs": [],
   "source": [
    "# Code?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "KY_NLP(S) - Student.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
