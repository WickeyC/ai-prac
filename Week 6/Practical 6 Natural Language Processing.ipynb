{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "### NLP\n",
    "\n",
    "▪ NLP is a machine learning technology that gives computers the ability to interpret, manipulate, and comprehend human language.\n",
    "\n",
    "<img src=\"nlp.jpg\" width=\"500\">\n",
    "\n",
    "### Structured Data vs. Unstructured Data\n",
    "\n",
    "▪ Structured data is standardized, clearly defined, and searchable data, while unstructured data is raw data of various forms.\n",
    "\n",
    "<img src=\"structured_vs_unstructured_data.png\" width=\"500\">\n",
    "\n",
    "### Garbage In, Garbage Out\n",
    "\n",
    "▪ Garbage in, garbage out is a concept common to computer science where the quality of output is determined by the quality of the input. \n",
    "\n",
    "<img src=\"garbage.png\" width=\"500\">\n",
    "\n",
    "### Text Preprocessing\n",
    "\n",
    "<img src=\"text_preprocessing.png\" width=\"700\">\n",
    "\n",
    "▪ Unstructured data must first be cleaned and pre-processed before analysis.\n",
    "\n",
    "textblob, nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35ABic3Ak02z"
   },
   "source": [
    "# NLTK and Preprocessing Techniques\n",
    "\n",
    "### NLP Toolkits\n",
    "\n",
    "▪ NLTK, which stands for Natural Language Toolkits, is a suite of libraries built for working with NLP in Python.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "▪ NLTK and NLTK dataset\n",
    "\n",
    "### Installing NLTK via Anaconda Prompt\n",
    "\n",
    "pip install nltk\n",
    "\n",
    "### Installing NLTK Dataset via Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# The following command downloads all data and models, and it will take awhile\n",
    "# Do this step only if nltk_data is not available on your pc\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLCozzHjk022"
   },
   "source": [
    "## Text Data\n",
    "\n",
    "▪ Text data is messy.\n",
    "\n",
    "▪ To analyze this data, it has to be preprocessed into clean text in a format that machine models can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvhK13m5k025"
   },
   "source": [
    "![](https://i.imgur.com/3L6x92C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data: Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"Hi Mr. Smith! I'm going to buy some vegetables \\\n",
    "(2 tomatoes and 4 cucumbers) from the store. Should I pick up some black-eyed peas as well?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLRudDeLk026"
   },
   "source": [
    "## Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMlRIFT8k026"
   },
   "outputs": [],
   "source": [
    "import re # Regular expression library\n",
    "\n",
    "clean_text = re.sub('[%s]' %(string.punctuation), '', original_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pynative.com/python-regex-special-sequences-and-character-classes/\n",
    "\n",
    "clean_text = re.sub('[^\\w\\s]','', original_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoaBcONOk027"
   },
   "source": [
    "## Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5e8Ot44k027"
   },
   "outputs": [],
   "source": [
    "# Removes all words containing digits\n",
    "clean_text = re.sub('\\d', '', clean_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = re.sub('[^A-Za-z\\s]','', original_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFSU428Ck026"
   },
   "source": [
    "## Covert Text to Lowercase\n",
    "\n",
    "<img src=\"vegetarian.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"The Nature's Vegetarian Restaurant located at Bangsar is a fantastic vegetarian restaurant.\"\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6hcxk1ak027"
   },
   "outputs": [],
   "source": [
    "clean_text = clean_text.lower()\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jBmgXb7k023"
   },
   "source": [
    "## Word Tokenization (original_text)\n",
    "\n",
    "▪ Tokenization is the process of breaking down a phrase, sentence, paragraph, or an entire text document into smaller units.\n",
    "\n",
    "<img src=\"tokenization.jpeg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"Hi Mr. Smith! I'm going to buy some vegetables \\\n",
    "(2 tomatoes and 4 cucumbers) from the store. Should I pick up some black-eyed peas as well?\"\n",
    "\n",
    "print(original_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#1 Word Tokenization with word_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYjeWTf0k023"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(original_text) \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#2 Word Tokenization with regexp_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize\n",
    "\n",
    "tokens = regexp_tokenize(original_text, pattern = '\\w+')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#3 Word Tokenization with split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = original_text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#4 Word Tokenization with Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.split(\"\\W+\", original_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.findall(\"\\w+\", original_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization (clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#1 Word Tokenization with word_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(clean_text) \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#2 Word Tokenization with regexp_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize\n",
    "\n",
    "tokens = regexp_tokenize(clean_text, pattern = '\\w+')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#3 Word Tokenization with split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = clean_text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#4 Word Tokenization with Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.split(\"\\W+\", clean_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.findall(\"\\w+\", clean_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzZ_SG_kk024"
   },
   "source": [
    "## Sentence Tokenization (original_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#1 Sentence Tokenization with sent_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wi834t7Ek024",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokens = sent_tokenize(original_text)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\#2 Sentence Tokenization with split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = original_text.split(\". \")\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = re.split(\"[?!.] \", original_text)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGP8X7cak027"
   },
   "source": [
    "## Remove Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2SzAuxbk028"
   },
   "source": [
    "![](https://i.imgur.com/T5RJXrX.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print English Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjdZhGrMk028"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "\n",
    "stop_words_2 = stopwords.words('chinese')\n",
    "print(stop_words_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Stop Words from the following Sentence\n",
    "\n",
    "<h3><center>Stopwords are a commonly used words that generally don’t contribute anything to the meaning of the text.</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the list of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.sort() # sorted()\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Language-X Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_3 = stopwords.words('danish')\n",
    "print(stop_words_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBTaQAS-k028"
   },
   "source": [
    "### Remove Stop Words from clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_x_stopwords = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "print(tokens_x_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKr2I0-Jk029"
   },
   "source": [
    "## Stemming\n",
    "\n",
    "![](https://i.imgur.com/9qllh8j.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming with LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_1 = ['Connects', 'Connecting', 'Connections', 'Connected', 'Connection', 'Connectings', 'Connect']\n",
    "words_2 = ['drive', 'drives', 'driver', 'drivers', 'driven', 'driving']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connects --> connect\n",
      "Connecting --> connect\n",
      "Connections --> connect\n",
      "Connected --> connect\n",
      "Connection --> connect\n",
      "Connectings --> connect\n",
      "Connect --> connect\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc_stemmer = LancasterStemmer()\n",
    "\n",
    "for word_1 in words_1:\n",
    "    print(word_1, \"-->\", lc_stemmer.stem(word_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "P-okFUv-k02-",
    "outputId": "d3f6ffe6-8c23-4adf-f9f6-e52e71e16045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive --> driv\n",
      "drives --> driv\n",
      "driver --> driv\n",
      "drivers --> driv\n",
      "driven --> driv\n",
      "driving --> driv\n"
     ]
    }
   ],
   "source": [
    "for word_2 in words_2:\n",
    "    print(word_2, \"-->\", lc_stemmer.stem(word_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming with PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connects --> connect\n",
      "Connecting --> connect\n",
      "Connections --> connect\n",
      "Connected --> connect\n",
      "Connection --> connect\n",
      "Connectings --> connect\n",
      "Connect --> connect\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pt_stemmer = PorterStemmer()\n",
    "\n",
    "for word_1 in words_1:\n",
    "    print(word_1, \"-->\", pt_stemmer.stem(word_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive ---> drive\n",
      "drives ---> drive\n",
      "driver ---> driver\n",
      "drivers ---> driver\n",
      "driven ---> driven\n",
      "driving ---> drive\n"
     ]
    }
   ],
   "source": [
    "for word_2 in words_2:\n",
    "    print(word_2, \"--->\", pt_stemmer.stem(word_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming with SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connects ---> connect\n",
      "Connecting ---> connect\n",
      "Connections ---> connect\n",
      "Connected ---> connect\n",
      "Connection ---> connect\n",
      "Connectings ---> connect\n",
      "Connect ---> connect\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sb_stemmer = SnowballStemmer(language = 'english')\n",
    "\n",
    "for word_1 in words_1:\n",
    "    print(word_1, \"--->\", sb_stemmer.stem(word_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive ---> drive\n",
      "drives ---> drive\n",
      "driver ---> driver\n",
      "drivers ---> driver\n",
      "driven ---> driven\n",
      "driving ---> drive\n"
     ]
    }
   ],
   "source": [
    "for word_2 in words_2:\n",
    "    print(word_2, \"--->\", sb_stemmer.stem(word_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeXl-dNTk02_",
    "outputId": "e2964c96-a7b0-40c7-a25c-cd4361cd29a0"
   },
   "source": [
    "## Lemmatization\n",
    "\n",
    "![](https://i.imgur.com/9qllh8j.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connects ---> Connects\n",
      "Connecting ---> Connecting\n",
      "Connections ---> Connections\n",
      "Connected ---> Connected\n",
      "Connection ---> Connection\n",
      "Connectings ---> Connectings\n",
      "Connect ---> Connect\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "wn_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word_1 in words_1:\n",
    "    print(word_1, \"--->\", wn_lemmatizer.lemmatize(word_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive ---> drive\n",
      "drives ---> drive\n",
      "driver ---> driver\n",
      "drivers ---> driver\n",
      "driven ---> driven\n",
      "driving ---> driving\n"
     ]
    }
   ],
   "source": [
    "for word_2 in words_2:\n",
    "    print(word_2, \"--->\", wn_lemmatizer.lemmatize(word_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with WordNetLemmatizer on clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_x_stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13592\\1857710105.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens_x_stopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens_x_stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "print(tokens_x_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_x_stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13592\\2458047624.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlemma_x_stopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mwn_lemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword_1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens_x_stopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemma_x_stopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens_x_stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "lemma_x_stopwords = [wn_lemmatizer.lemmatize(word_1) for word_1 in tokens_x_stopwords]\n",
    "\n",
    "print(lemma_x_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icdQoHrrk03A"
   },
   "source": [
    "## Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgyUyDZfk02_"
   },
   "source": [
    "![](https://i.imgur.com/8edVsCR.png)\n",
    "\n",
    "do at very beginning since meaning might change after clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nltk.help.upenn_tagset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging on Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NiFKoaQ2k03A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'NNP'), ('Smith', 'NNP'), ('lives', 'VBZ'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "text_1 = \"James Smith lives in the United States.\"\n",
    "\n",
    "tokens = pos_tag(word_tokenize(text_1))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging on original_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Mr. Smith! I'm going to buy some vegetables (2 tomatoes and 4 cucumbers) from the store. Should I pick up some black-eyed peas as well?\n"
     ]
    }
   ],
   "source": [
    "print(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi', 'NNP'), ('Mr.', 'NNP'), ('Smith', 'NNP'), ('!', '.'), ('I', 'PRP'), (\"'m\", 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('buy', 'VB'), ('some', 'DT'), ('vegetables', 'NNS'), ('(', '('), ('2', 'CD'), ('tomatoes', 'NNS'), ('and', 'CC'), ('4', 'CD'), ('cucumbers', 'NNS'), (')', ')'), ('from', 'IN'), ('the', 'DT'), ('store', 'NN'), ('.', '.'), ('Should', 'MD'), ('I', 'PRP'), ('pick', 'VB'), ('up', 'RP'), ('some', 'DT'), ('black-eyed', 'JJ'), ('peas', 'NNS'), ('as', 'IN'), ('well', 'RB'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "tokens = pos_tag(word_tokenize(original_text))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging on clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemma_x_stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13592\\2573733712.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemma_x_stopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lemma_x_stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = pos_tag(lemma_x_stopwords)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRjifONik03A"
   },
   "source": [
    "## Chunking\n",
    "\n",
    "▪ Chunking is a step following POS tagging and structuring the sentence in \"chunks\" by identifying continuous words that can be grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6xqPDx0k03A"
   },
   "outputs": [],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "text_1 = \"James Smith lives in the United States.\"\n",
    "tokens = pos_tag(word_tokenize(text_1))\n",
    "\n",
    "# This extracts entities from the list of words\n",
    "result = ne_chunk(tokens) \n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = \"The Nature's Vegetarian Restaurant located at Bangsar is a fantastic vegetarian restaurant.\"\n",
    "tokens = pos_tag(word_tokenize(text_2))\n",
    "\n",
    "result = ne_chunk(tokens) \n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3mdUZ4Mk03A"
   },
   "source": [
    "## Compound Term Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1Qw0PuWk03B"
   },
   "source": [
    "![](https://i.imgur.com/q1WuWai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH4qmhFVk03B"
   },
   "source": [
    "### Compound Term Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer \n",
    "\n",
    "mwe_tokenizer = MWETokenizer([('James', 'Smith'), ('United', 'States')])\n",
    "\n",
    "text_1 = \"James Smith lives in the United States.\"\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text_1))\n",
    "mwe_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokenizer = MWETokenizer([(\"Nature's\", \"Vegetarian\", \"Restaurant\"), ('Subang', 'Jaya')])\n",
    "\n",
    "text_3 = \"The Nature's Vegetarian Restaurant located at Subang Jaya is a fantastic vegetarian restaurant.\"\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text_3))\n",
    "mwe_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Numpy Functionality\n",
    "\n",
    "▪ NumPy, which stands for Numerical Python, is a Python library used for working with arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an ndarray (2D Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 6, 8])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr_1d = np.array([2, 4, 6, 8]) \n",
    "arr_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(arr_1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.85780906,  0.31365262, -0.70057474, -0.30541553],\n",
       "       [ 1.55990202,  0.14329982, -0.27839342,  0.62009336],\n",
       "       [ 0.59011357,  0.68103976, -0.11370023,  0.44723942],\n",
       "       [-0.51140401,  0.57171169,  0.0826537 , -0.03379511],\n",
       "       [ 2.73911057, -1.66741524,  0.70033702,  0.84138539],\n",
       "       [-0.43435138,  2.11636159, -0.88252768,  1.76292792]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_2d = np.random.randn(6, 4)\n",
    "arr_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(arr_2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oqFOp9nk03B"
   },
   "source": [
    "## Basic Pandas Functionality\n",
    "\n",
    "▪ Pandas stands for \"Python Data Analysis Library\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot21kY7mk03B"
   },
   "source": [
    "![](https://i.imgur.com/HpgLFOT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe\n",
    "\n",
    "▪ A dataframe is a 2-dimensional labeled data structure with columns of potentially different types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.857809</td>\n",
       "      <td>0.313653</td>\n",
       "      <td>-0.700575</td>\n",
       "      <td>-0.305416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.559902</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>-0.278393</td>\n",
       "      <td>0.620093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.590114</td>\n",
       "      <td>0.681040</td>\n",
       "      <td>-0.113700</td>\n",
       "      <td>0.447239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.511404</td>\n",
       "      <td>0.571712</td>\n",
       "      <td>0.082654</td>\n",
       "      <td>-0.033795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.739111</td>\n",
       "      <td>-1.667415</td>\n",
       "      <td>0.700337</td>\n",
       "      <td>0.841385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.434351</td>\n",
       "      <td>2.116362</td>\n",
       "      <td>-0.882528</td>\n",
       "      <td>1.762928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0  0.857809  0.313653 -0.700575 -0.305416\n",
       "1  1.559902  0.143300 -0.278393  0.620093\n",
       "2  0.590114  0.681040 -0.113700  0.447239\n",
       "3 -0.511404  0.571712  0.082654 -0.033795\n",
       "4  2.739111 -1.667415  0.700337  0.841385\n",
       "5 -0.434351  2.116362 -0.882528  1.762928"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(arr_2d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Labels of Rows and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Labels to Rows and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.857809</td>\n",
       "      <td>0.313653</td>\n",
       "      <td>-0.700575</td>\n",
       "      <td>-0.305416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.559902</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>-0.278393</td>\n",
       "      <td>0.620093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.590114</td>\n",
       "      <td>0.681040</td>\n",
       "      <td>-0.113700</td>\n",
       "      <td>0.447239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.511404</td>\n",
       "      <td>0.571712</td>\n",
       "      <td>0.082654</td>\n",
       "      <td>-0.033795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.739111</td>\n",
       "      <td>-1.667415</td>\n",
       "      <td>0.700337</td>\n",
       "      <td>0.841385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.434351</td>\n",
       "      <td>2.116362</td>\n",
       "      <td>-0.882528</td>\n",
       "      <td>1.762928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C         D\n",
       "0  0.857809  0.313653 -0.700575 -0.305416\n",
       "1  1.559902  0.143300 -0.278393  0.620093\n",
       "2  0.590114  0.681040 -0.113700  0.447239\n",
       "3 -0.511404  0.571712  0.082654 -0.033795\n",
       "4  2.739111 -1.667415  0.700337  0.841385\n",
       "5 -0.434351  2.116362 -0.882528  1.762928"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['A', 'B', 'C', 'D']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rec_1</th>\n",
       "      <td>0.857809</td>\n",
       "      <td>0.313653</td>\n",
       "      <td>-0.700575</td>\n",
       "      <td>-0.305416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rec_2</th>\n",
       "      <td>1.559902</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>-0.278393</td>\n",
       "      <td>0.620093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rec_3</th>\n",
       "      <td>0.590114</td>\n",
       "      <td>0.681040</td>\n",
       "      <td>-0.113700</td>\n",
       "      <td>0.447239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rec_4</th>\n",
       "      <td>-0.511404</td>\n",
       "      <td>0.571712</td>\n",
       "      <td>0.082654</td>\n",
       "      <td>-0.033795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rec_5</th>\n",
       "      <td>2.739111</td>\n",
       "      <td>-1.667415</td>\n",
       "      <td>0.700337</td>\n",
       "      <td>0.841385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rec_6</th>\n",
       "      <td>-0.434351</td>\n",
       "      <td>2.116362</td>\n",
       "      <td>-0.882528</td>\n",
       "      <td>1.762928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              A         B         C         D\n",
       "Rec_1  0.857809  0.313653 -0.700575 -0.305416\n",
       "Rec_2  1.559902  0.143300 -0.278393  0.620093\n",
       "Rec_3  0.590114  0.681040 -0.113700  0.447239\n",
       "Rec_4 -0.511404  0.571712  0.082654 -0.033795\n",
       "Rec_5  2.739111 -1.667415  0.700337  0.841385\n",
       "Rec_6 -0.434351  2.116362 -0.882528  1.762928"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index = ['Rec_1', 'Rec_2', 'Rec_3', 'Rec_4', 'Rec_5', 'Rec_6']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cookie_reviews.csv\n",
    "\n",
    "### Reading data from cookie_reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A368Z46FIKHSEZ</td>\n",
       "      <td>5</td>\n",
       "      <td>I love these cookies!  Not only are they healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1JAPP1CXRG57A</td>\n",
       "      <td>5</td>\n",
       "      <td>Quaker Soft Baked Oatmeal Cookies with raisins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2Z9JNXPIEL2B9</td>\n",
       "      <td>5</td>\n",
       "      <td>I am usually not a huge fan of oatmeal cookies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A31CYJQO3FL586</td>\n",
       "      <td>5</td>\n",
       "      <td>I participated in a product review that includ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2KXQ2EKFF3K2G</td>\n",
       "      <td>5</td>\n",
       "      <td>My kids loved these. I was very pleased to giv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>A366PSH7KFLRPB</td>\n",
       "      <td>5</td>\n",
       "      <td>I loved these cookies and so did my kids. You ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>A2KV6EYQPKJRR5</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a great tasting cookie. It is very sof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>A3O7REI0OSV89M</td>\n",
       "      <td>4</td>\n",
       "      <td>These are great for a quick snack! They are sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>A9JS5GQQ6GIQT</td>\n",
       "      <td>5</td>\n",
       "      <td>I love the Quaker soft baked cookies.  The rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>AMAVEZAGCH52H</td>\n",
       "      <td>5</td>\n",
       "      <td>This cookie is really good and works really we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>913 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id  stars                                            reviews\n",
       "0    A368Z46FIKHSEZ      5  I love these cookies!  Not only are they healt...\n",
       "1    A1JAPP1CXRG57A      5  Quaker Soft Baked Oatmeal Cookies with raisins...\n",
       "2    A2Z9JNXPIEL2B9      5  I am usually not a huge fan of oatmeal cookies...\n",
       "3    A31CYJQO3FL586      5  I participated in a product review that includ...\n",
       "4    A2KXQ2EKFF3K2G      5  My kids loved these. I was very pleased to giv...\n",
       "..              ...    ...                                                ...\n",
       "908  A366PSH7KFLRPB      5  I loved these cookies and so did my kids. You ...\n",
       "909  A2KV6EYQPKJRR5      5  This is a great tasting cookie. It is very sof...\n",
       "910  A3O7REI0OSV89M      4  These are great for a quick snack! They are sa...\n",
       "911   A9JS5GQQ6GIQT      5  I love the Quaker soft baked cookies.  The rea...\n",
       "912   AMAVEZAGCH52H      5  This cookie is really good and works really we...\n",
       "\n",
       "[913 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cookie_reviews.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A368Z46FIKHSEZ</td>\n",
       "      <td>5</td>\n",
       "      <td>I love these cookies!  Not only are they healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1JAPP1CXRG57A</td>\n",
       "      <td>5</td>\n",
       "      <td>Quaker Soft Baked Oatmeal Cookies with raisins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2Z9JNXPIEL2B9</td>\n",
       "      <td>5</td>\n",
       "      <td>I am usually not a huge fan of oatmeal cookies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A31CYJQO3FL586</td>\n",
       "      <td>5</td>\n",
       "      <td>I participated in a product review that includ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2KXQ2EKFF3K2G</td>\n",
       "      <td>5</td>\n",
       "      <td>My kids loved these. I was very pleased to giv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  stars                                            reviews\n",
       "0  A368Z46FIKHSEZ      5  I love these cookies!  Not only are they healt...\n",
       "1  A1JAPP1CXRG57A      5  Quaker Soft Baked Oatmeal Cookies with raisins...\n",
       "2  A2Z9JNXPIEL2B9      5  I am usually not a huge fan of oatmeal cookies...\n",
       "3  A31CYJQO3FL586      5  I participated in a product review that includ...\n",
       "4  A2KXQ2EKFF3K2G      5  My kids loved these. I was very pleased to giv..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>A366PSH7KFLRPB</td>\n",
       "      <td>5</td>\n",
       "      <td>I loved these cookies and so did my kids. You ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>A2KV6EYQPKJRR5</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a great tasting cookie. It is very sof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>A3O7REI0OSV89M</td>\n",
       "      <td>4</td>\n",
       "      <td>These are great for a quick snack! They are sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>A9JS5GQQ6GIQT</td>\n",
       "      <td>5</td>\n",
       "      <td>I love the Quaker soft baked cookies.  The rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>AMAVEZAGCH52H</td>\n",
       "      <td>5</td>\n",
       "      <td>This cookie is really good and works really we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id  stars                                            reviews\n",
       "908  A366PSH7KFLRPB      5  I loved these cookies and so did my kids. You ...\n",
       "909  A2KV6EYQPKJRR5      5  This is a great tasting cookie. It is very sof...\n",
       "910  A3O7REI0OSV89M      4  These are great for a quick snack! They are sa...\n",
       "911   A9JS5GQQ6GIQT      5  I love the Quaker soft baked cookies.  The rea...\n",
       "912   AMAVEZAGCH52H      5  This cookie is really good and works really we..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A368Z46FIKHSEZ</td>\n",
       "      <td>5</td>\n",
       "      <td>I love these cookies!  Not only are they healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1JAPP1CXRG57A</td>\n",
       "      <td>5</td>\n",
       "      <td>Quaker Soft Baked Oatmeal Cookies with raisins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2Z9JNXPIEL2B9</td>\n",
       "      <td>5</td>\n",
       "      <td>I am usually not a huge fan of oatmeal cookies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A31CYJQO3FL586</td>\n",
       "      <td>5</td>\n",
       "      <td>I participated in a product review that includ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2KXQ2EKFF3K2G</td>\n",
       "      <td>5</td>\n",
       "      <td>My kids loved these. I was very pleased to giv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A2U5TAIAQ675BL</td>\n",
       "      <td>5</td>\n",
       "      <td>I really enjoyed these individually wrapped bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A1R4PIBZBD3NZ0</td>\n",
       "      <td>4</td>\n",
       "      <td>I was surprised at how soft the cookie was. I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A1ECQ8LJMXG4WI</td>\n",
       "      <td>5</td>\n",
       "      <td>Filled with oats and raisins you'll love this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A3MSG4E5MLI1XP</td>\n",
       "      <td>5</td>\n",
       "      <td>I was recently given a complimentary \"vox box\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A3BUDUV9GORLWH</td>\n",
       "      <td>5</td>\n",
       "      <td>the best and freshest cookie that comes in a p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  stars                                            reviews\n",
       "0  A368Z46FIKHSEZ      5  I love these cookies!  Not only are they healt...\n",
       "1  A1JAPP1CXRG57A      5  Quaker Soft Baked Oatmeal Cookies with raisins...\n",
       "2  A2Z9JNXPIEL2B9      5  I am usually not a huge fan of oatmeal cookies...\n",
       "3  A31CYJQO3FL586      5  I participated in a product review that includ...\n",
       "4  A2KXQ2EKFF3K2G      5  My kids loved these. I was very pleased to giv...\n",
       "5  A2U5TAIAQ675BL      5  I really enjoyed these individually wrapped bi...\n",
       "6  A1R4PIBZBD3NZ0      4  I was surprised at how soft the cookie was. I ...\n",
       "7  A1ECQ8LJMXG4WI      5  Filled with oats and raisins you'll love this ...\n",
       "8  A3MSG4E5MLI1XP      5  I was recently given a complimentary \"vox box\"...\n",
       "9  A3BUDUV9GORLWH      5  the best and freshest cookie that comes in a p..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>A55PK06Q6AKFY</td>\n",
       "      <td>4</td>\n",
       "      <td>These cookies are soft and delicious.  Possibl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>A1YJMG0QJXZLD4</td>\n",
       "      <td>5</td>\n",
       "      <td>I cannot say these taste like home made but th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>A1W0EK0033YVGP</td>\n",
       "      <td>5</td>\n",
       "      <td>this cookie is super soft and chewy.i love it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>AFJFINIKFOFSB</td>\n",
       "      <td>3</td>\n",
       "      <td>These cookies are reasonably tasty without bei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>AQUMNB8YWE595</td>\n",
       "      <td>5</td>\n",
       "      <td>THIS IS A FAB PRODUCT SOFT AND CHEWY YOU CANT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>A366PSH7KFLRPB</td>\n",
       "      <td>5</td>\n",
       "      <td>I loved these cookies and so did my kids. You ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>A2KV6EYQPKJRR5</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a great tasting cookie. It is very sof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>A3O7REI0OSV89M</td>\n",
       "      <td>4</td>\n",
       "      <td>These are great for a quick snack! They are sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>A9JS5GQQ6GIQT</td>\n",
       "      <td>5</td>\n",
       "      <td>I love the Quaker soft baked cookies.  The rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>AMAVEZAGCH52H</td>\n",
       "      <td>5</td>\n",
       "      <td>This cookie is really good and works really we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id  stars                                            reviews\n",
       "903   A55PK06Q6AKFY      4  These cookies are soft and delicious.  Possibl...\n",
       "904  A1YJMG0QJXZLD4      5  I cannot say these taste like home made but th...\n",
       "905  A1W0EK0033YVGP      5  this cookie is super soft and chewy.i love it ...\n",
       "906   AFJFINIKFOFSB      3  These cookies are reasonably tasty without bei...\n",
       "907   AQUMNB8YWE595      5  THIS IS A FAB PRODUCT SOFT AND CHEWY YOU CANT ...\n",
       "908  A366PSH7KFLRPB      5  I loved these cookies and so did my kids. You ...\n",
       "909  A2KV6EYQPKJRR5      5  This is a great tasting cookie. It is very sof...\n",
       "910  A3O7REI0OSV89M      4  These are great for a quick snack! They are sa...\n",
       "911   A9JS5GQQ6GIQT      5  I love the Quaker soft baked cookies.  The rea...\n",
       "912   AMAVEZAGCH52H      5  This cookie is really good and works really we..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the Labels of All Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['user_id', 'stars', 'reviews'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print All Values of a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      A368Z46FIKHSEZ\n",
       "1      A1JAPP1CXRG57A\n",
       "2      A2Z9JNXPIEL2B9\n",
       "3      A31CYJQO3FL586\n",
       "4      A2KXQ2EKFF3K2G\n",
       "            ...      \n",
       "908    A366PSH7KFLRPB\n",
       "909    A2KV6EYQPKJRR5\n",
       "910    A3O7REI0OSV89M\n",
       "911     A9JS5GQQ6GIQT\n",
       "912     AMAVEZAGCH52H\n",
       "Name: user_id, Length: 913, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      I love these cookies!  Not only are they healt...\n",
       "1      Quaker Soft Baked Oatmeal Cookies with raisins...\n",
       "2      I am usually not a huge fan of oatmeal cookies...\n",
       "3      I participated in a product review that includ...\n",
       "4      My kids loved these. I was very pleased to giv...\n",
       "                             ...                        \n",
       "908    I loved these cookies and so did my kids. You ...\n",
       "909    This is a great tasting cookie. It is very sof...\n",
       "910    These are great for a quick snack! They are sa...\n",
       "911    I love the Quaker soft baked cookies.  The rea...\n",
       "912    This cookie is really good and works really we...\n",
       "Name: reviews, Length: 913, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Slicing\n",
    "\n",
    "▪ Python iloc() function is used for integer-location based indexing / selection by position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "fYG95Kw0k03C"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                                       A368Z46FIKHSEZ\n",
       "stars                                                      5\n",
       "reviews    I love these cookies!  Not only are they healt...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0] # first row of data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                                        AMAVEZAGCH52H\n",
       "stars                                                      5\n",
       "reviews    This cookie is really good and works really we...\n",
       "Name: 912, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[-1] # last row of data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      A368Z46FIKHSEZ\n",
       "1      A1JAPP1CXRG57A\n",
       "2      A2Z9JNXPIEL2B9\n",
       "3      A31CYJQO3FL586\n",
       "4      A2KXQ2EKFF3K2G\n",
       "            ...      \n",
       "908    A366PSH7KFLRPB\n",
       "909    A2KV6EYQPKJRR5\n",
       "910    A3O7REI0OSV89M\n",
       "911     A9JS5GQQ6GIQT\n",
       "912     AMAVEZAGCH52H\n",
       "Name: user_id, Length: 913, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,0] # first column of data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      I love these cookies!  Not only are they healt...\n",
       "1      Quaker Soft Baked Oatmeal Cookies with raisins...\n",
       "2      I am usually not a huge fan of oatmeal cookies...\n",
       "3      I participated in a product review that includ...\n",
       "4      My kids loved these. I was very pleased to giv...\n",
       "                             ...                        \n",
       "908    I loved these cookies and so did my kids. You ...\n",
       "909    This is a great tasting cookie. It is very sof...\n",
       "910    These are great for a quick snack! They are sa...\n",
       "911    I love the Quaker soft baked cookies.  The rea...\n",
       "912    This cookie is really good and works really we...\n",
       "Name: reviews, Length: 913, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,-1] # last column of data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0, 1] # first row, second column of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A368Z46FIKHSEZ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1JAPP1CXRG57A</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2Z9JNXPIEL2B9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A31CYJQO3FL586</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  stars\n",
       "0  A368Z46FIKHSEZ      5\n",
       "1  A1JAPP1CXRG57A      5\n",
       "2  A2Z9JNXPIEL2B9      5\n",
       "3  A31CYJQO3FL586      5"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0:4, 0:2] # first 4 rows and first 2 columns of data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73NlJvM_k03D"
   },
   "source": [
    "## Preprocessing Exercise: cookie_reviews.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhpoyqpfk03E"
   },
   "source": [
    "#### Question 1: Determine how many reviews there are in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9Dz0atok03E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l63g7P8fk03E"
   },
   "source": [
    "#### Question 2: Determine the percentage of 1, 2, 3, 4 and 5 star reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BKYT60ck03E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9frnQ720k03E"
   },
   "source": [
    "#### Question 3: Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqY6OLimk03E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uubKAy_ak03F"
   },
   "source": [
    "#### Question 4: Change to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be2IxAEfk03F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mTwPsiFk03F"
   },
   "source": [
    "#### Question 5: Perform stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjFvaoJzk03F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv9e74jDk03F"
   },
   "source": [
    "## Text Similarity Measures\n",
    "\n",
    "▪ To measure distance between 2 strings.\n",
    "\n",
    "<img src=\"similarity.png\" width=\"700\">\n",
    "\n",
    "▪ Some examples of its application include information retrieval, text classification, document clustering, and topic modeling.\n",
    "\n",
    "### Levenshtein distance\n",
    "\n",
    "▪ **Levenshtein distance** is one way to measure the word similarity. \n",
    "\n",
    "▪ Minimum number of operations to get from one word to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaG7KOEik03F"
   },
   "source": [
    "![](https://i.imgur.com/FkdJmPi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1WK5fP5k03G"
   },
   "source": [
    "# TextBlob\n",
    "\n",
    "▪ Other than NLTK, TextBlob is another Python library for processing textual data.\n",
    "\n",
    "▪ TextBlob capabilities: Tokenization, Parts of speech tagging, Sentiment analysis, Spell check, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUlBT04yk03G"
   },
   "source": [
    "## TextBlob Demo: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1571,
     "status": "error",
     "timestamp": 1625705228446,
     "user": {
      "displayName": "CHING PANG GOH",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhAtxw1eyRJoJoQwsqDiWRYXwwRLKTenuEJ5Ge2vQ=s64",
      "userId": "03150219032412111071"
     },
     "user_tz": -480
    },
    "id": "lYml12fGk03G",
    "outputId": "d7a4566b-87a1-49a9-9ea6-3baa8094f151"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13504\\1360118352.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#pip install textblob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmy_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"We're moving from NLTK to TextBlob. How fun!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmy_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "#pip install textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "my_text = TextBlob(\"We're moving from NLTK to TextBlob. How fun!\")\n",
    "my_text.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFMRGAVck03G"
   },
   "source": [
    "## TextBlob Demo: Spell Check\n",
    "\n",
    "▪ The correct() function calculates the Levenshtein distance between the word \"graat\" and all words in its word list of the words with the smallest Levenshtein distance, it outputs the most popular word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erc2XHNJk03G",
    "outputId": "4d6a0d9b-bf42-4845-b44d-ddf5c133d05b"
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(\"I'm graat at speling.\")\n",
    "print(blob.correct()) # print function requires Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGGHwF-_k03H"
   },
   "source": [
    "## TextBlob Demo: Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBPNmxdLk03H",
    "outputId": "83ac4941-a17d-4fea-e0af-1d64863b88a3"
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(\"John hits the ball.\")\n",
    "for words, tag in blob.tags:\n",
    "    print (words, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IjrZlWlk03H"
   },
   "source": [
    "## TextBlob Demo: Language Translation\n",
    "\n",
    "▪ Textblob uses Google Translate as its translation engine\n",
    "\n",
    "https://thinkinfi.com/natural-language-processing-using-textblob/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0Jk18Vjk03I",
    "outputId": "7a57f130-4a65-45d9-c7b5-6f0b0b40324e"
   },
   "outputs": [],
   "source": [
    "word = TextBlob(\"Bonjour, comment allez-vous\")\n",
    "word.translate(from_lang = 'fr', to = 'cn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word.translate(from_lang = 'fr', to = 'zh-CN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9P_IjW4ok03I"
   },
   "source": [
    "## Text Format for Analysis: Count Vectorizer\n",
    "\n",
    "![](https://i.imgur.com/OQDeQlb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vt8xtmfCk028"
   },
   "source": [
    "### Features Extraction and CountVectorizer\n",
    "\n",
    "▪ Feature extraction is the process of transforming textual data into numerical data.\n",
    "\n",
    "▪ CountVectorizer is a tool used to vectorize text data by converting it into a matrix of token counts.\n",
    "\n",
    "![](Count-Vectorization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Create a DataFrame from original_text using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = [\"Hi Mr. Smith! I'm going to buy some vegetables \\\n",
    "(2 tomatoes and 4 cucumbers) from the store. Should I pick up some black-eyed peas as well?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "3lBhjqJGk029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 7)\t1\n",
      "\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 14)\t2\n",
      "  (0, 20)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 21)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "           \n",
    "# Incorporate stop words when creating the count vectorizer\n",
    "cv = CountVectorizer(stop_words = 'english') \n",
    "\n",
    "X = cv.fit_transform(original_text)\n",
    "print(X)\n",
    "print()\n",
    "\n",
    "cv2 = CountVectorizer() \n",
    "\n",
    "Y = cv2.fit_transform(original_text)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hi': 5,\n",
       " 'mr': 6,\n",
       " 'smith': 9,\n",
       " 'going': 4,\n",
       " 'buy': 1,\n",
       " 'vegetables': 12,\n",
       " 'tomatoes': 11,\n",
       " 'cucumbers': 2,\n",
       " 'store': 10,\n",
       " 'pick': 8,\n",
       " 'black': 0,\n",
       " 'eyed': 3,\n",
       " 'peas': 7}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   10  11  12\n",
       "0   1   1   1   1   1   1   1   1   1   1   1   1   1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X.toarray())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>black</th>\n",
       "      <th>buy</th>\n",
       "      <th>cucumbers</th>\n",
       "      <th>eyed</th>\n",
       "      <th>going</th>\n",
       "      <th>hi</th>\n",
       "      <th>mr</th>\n",
       "      <th>peas</th>\n",
       "      <th>pick</th>\n",
       "      <th>smith</th>\n",
       "      <th>store</th>\n",
       "      <th>tomatoes</th>\n",
       "      <th>vegetables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   black  buy  cucumbers  eyed  going  hi  mr  peas  pick  smith  store  \\\n",
       "0      1    1          1     1      1   1   1     1     1      1      1   \n",
       "\n",
       "   tomatoes  vegetables  \n",
       "0         1           1  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X.toarray(), columns = cv.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Create a DataFrame from clean_text using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemma_x_stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13592\\1656727497.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemma_x_stopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lemma_x_stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "print(lemma_x_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemma_x_stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13592\\3798044718.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemma_x_stopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lemma_x_stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "text = [\" \".join(lemma_x_stopwords)]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13592\\549283500.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer() \n",
    "\n",
    "X = cv.fit_transform(text)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns = cv.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Create a DataFrame from the following Corpus using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "71HqDC9fk03I",
    "outputId": "f788a730-e2c0-445a-94da-645b67ce85a2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>fun</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  fun  is  one  second  the  third  this\n",
       "0    0         1      1    0   1    0       0    1      0     1\n",
       "1    0         1      0    0   1    0       1    1      0     1\n",
       "2    1         0      0    1   1    2       0    1      1     0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['This is the first document.', \n",
    "          'This is the second document.', \n",
    "          'And the third one. One is fun.']\n",
    "\n",
    "cv = CountVectorizer() \n",
    "\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns = cv.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Create a DataFrame from the following Corpus using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "iq-Cszhdk03J",
    "outputId": "bc2dd627-1839-4101-d733-a9842de3e019"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chai</th>\n",
       "      <th>chocolate</th>\n",
       "      <th>encoding</th>\n",
       "      <th>hot</th>\n",
       "      <th>latte</th>\n",
       "      <th>make</th>\n",
       "      <th>milk</th>\n",
       "      <th>sale</th>\n",
       "      <th>sun</th>\n",
       "      <th>today</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chai  chocolate  encoding  hot  latte  make  milk  sale  sun  today  \\\n",
       "0     0          0         0    1      0     0     0     0    1      0   \n",
       "1     0          1         0    1      0     1     1     0    0      0   \n",
       "2     0          0         1    1      0     0     0     0    0      0   \n",
       "3     1          0         0    0      1     0     1     0    0      0   \n",
       "4     0          0         0    1      0     0     0     1    0      1   \n",
       "\n",
       "   weather  \n",
       "0        1  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['The weather is hot under the sun',\n",
    "          'I make my hot chocolate with milk',\n",
    "          'One hot encoding',\n",
    "          'I will have a chai latte with milk',\n",
    "          'There is a hot sale today']\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english') \n",
    "\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "df = pd.DataFrame(X, columns = cv.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GefIA4Jk03J"
   },
   "source": [
    "## Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSF6LaN3k03J"
   },
   "source": [
    "![](https://i.imgur.com/PyirXsy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reTZir8kk03K"
   },
   "source": [
    "### Measuring Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "DZbnjB3Hk03K",
    "outputId": "c8186453-7535-4086-9499-e394d20da6fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'B'), ('A', 'C'), ('A', 'D'), ('B', 'C'), ('B', 'D'), ('C', 'D')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "pairs = list(combinations(['A', 'B', 'C', 'D'], 2))\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# range() returns an immutable sequence of numbers that can be easily converted to lists\n",
    "x = list(range(5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "tLI6INUvk03K",
    "outputId": "7df91d90-1c96-4ab5-bc71-143d623be916"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (3, 4)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the cosine similarity between all combinations of documents\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# list all combinations of the 5 sentences in pairs, in terms of indexes\n",
    "# (0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), ..., (3,4)\n",
    "pairs = list(combinations(range(len(corpus)), 2)) \n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The weather is hot under the sun', 'I make my hot chocolate with milk'),\n",
       " ('The weather is hot under the sun', 'One hot encoding'),\n",
       " ('The weather is hot under the sun', 'I will have a chai latte with milk'),\n",
       " ('The weather is hot under the sun', 'There is a hot sale today'),\n",
       " ('I make my hot chocolate with milk', 'One hot encoding'),\n",
       " ('I make my hot chocolate with milk', 'I will have a chai latte with milk'),\n",
       " ('I make my hot chocolate with milk', 'There is a hot sale today'),\n",
       " ('One hot encoding', 'I will have a chai latte with milk'),\n",
       " ('One hot encoding', 'There is a hot sale today'),\n",
       " ('I will have a chai latte with milk', 'There is a hot sale today')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combos = [(corpus[a_index], corpus[b_index]) for (a_index, b_index) in pairs]\n",
    "combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[0.40824829]]),\n",
       "  ('The weather is hot under the sun', 'One hot encoding')),\n",
       " (array([[0.40824829]]), ('One hot encoding', 'There is a hot sale today')),\n",
       " (array([[0.35355339]]),\n",
       "  ('I make my hot chocolate with milk', 'One hot encoding')),\n",
       " (array([[0.33333333]]),\n",
       "  ('The weather is hot under the sun', 'There is a hot sale today')),\n",
       " (array([[0.28867513]]),\n",
       "  ('The weather is hot under the sun', 'I make my hot chocolate with milk')),\n",
       " (array([[0.28867513]]),\n",
       "  ('I make my hot chocolate with milk', 'There is a hot sale today')),\n",
       " (array([[0.28867513]]),\n",
       "  ('I make my hot chocolate with milk', 'I will have a chai latte with milk')),\n",
       " (array([[0.]]),\n",
       "  ('The weather is hot under the sun', 'I will have a chai latte with milk')),\n",
       " (array([[0.]]), ('One hot encoding', 'I will have a chai latte with milk')),\n",
       " (array([[0.]]),\n",
       "  ('I will have a chai latte with milk', 'There is a hot sale today'))]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
    "results = [cosine_similarity([X[a_index]], [X[b_index]]) for (a_index, b_index) in pairs]\n",
    "sorted(zip(results, combos), reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Which Two Documents are Most Similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63h3cPxxk03N"
   },
   "source": [
    "![](https://i.imgur.com/jrfN6Jj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdYuTyMkk03N"
   },
   "source": [
    "![](https://i.imgur.com/BI8XP92.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysNlUOUck03N"
   },
   "source": [
    "![](https://i.imgur.com/3IbfQXT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaYfQGbok03N"
   },
   "source": [
    "![](https://i.imgur.com/pnNqzql.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer vs. TfidfVectorizer\n",
    "\n",
    "▪ Original documents\n",
    "\n",
    "![](table2.png)\n",
    "\n",
    "▪ Documents with stopwords removed\n",
    "\n",
    "![](table1.png)\n",
    "\n",
    "▪ Feature extraction with CountVectorizer\n",
    "\n",
    "![](table3.png)\n",
    "\n",
    "Feature extraction with TfidfVectorizer\n",
    "\n",
    "![](table4.png)\n",
    "\n",
    "https://medium.com/codex/document-indexing-using-tf-idf-189afd04a9fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "SOxJucLNk03N",
    "outputId": "bbf2c1d1-89ec-4a6a-fa3a-2ab044b7e427"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>fun</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  fun  is  one  second  the  third  this\n",
       "0    0         1      1    0   1    0       0    1      0     1\n",
       "1    0         1      0    0   1    0       1    1      0     1\n",
       "2    1         0      0    1   1    2       0    1      1     0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['This is the first document.',\n",
    "          'This is the second document.',\n",
    "          'And the third one. One is fun.']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "df = pd.DataFrame(X, columns=cv.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "DwU3Q63Bk03O",
    "outputId": "5f405d59-5a48-4988-89c3-1cc8dcba43f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>fun</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.450145</td>\n",
       "      <td>0.591887</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.450145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.450145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.591887</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.450145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.212876</td>\n",
       "      <td>0.72086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212876</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       and  document     first      fun        is      one    second  \\\n",
       "0  0.00000  0.450145  0.591887  0.00000  0.349578  0.00000  0.000000   \n",
       "1  0.00000  0.450145  0.000000  0.00000  0.349578  0.00000  0.591887   \n",
       "2  0.36043  0.000000  0.000000  0.36043  0.212876  0.72086  0.000000   \n",
       "\n",
       "        the    third      this  \n",
       "0  0.349578  0.00000  0.450145  \n",
       "1  0.349578  0.00000  0.450145  \n",
       "2  0.212876  0.36043  0.000000  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cv_tfidf = TfidfVectorizer()\n",
    "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
    "df_tfidf = pd.DataFrame(X_tfidf, columns=cv_tfidf.get_feature_names_out())\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYpkjzzik03O"
   },
   "source": [
    "![](https://i.imgur.com/xlJibKw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0dZnaIzk03O"
   },
   "source": [
    "### Document Similarity: Example with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "2Jwp0N7ok03O",
    "outputId": "66ab19c6-dd92-415e-b32c-63d8e477ff8e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chai</th>\n",
       "      <th>chocolate</th>\n",
       "      <th>encoding</th>\n",
       "      <th>hot</th>\n",
       "      <th>latte</th>\n",
       "      <th>make</th>\n",
       "      <th>milk</th>\n",
       "      <th>sale</th>\n",
       "      <th>sun</th>\n",
       "      <th>today</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6569</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.580423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.580423</td>\n",
       "      <td>0.468282</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.871247</td>\n",
       "      <td>0.490845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.614189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.614189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6569</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6569</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       chai  chocolate  encoding       hot     latte      make      milk  \\\n",
       "0  0.000000   0.000000  0.000000  0.370086  0.000000  0.000000  0.000000   \n",
       "1  0.000000   0.580423  0.000000  0.327000  0.000000  0.580423  0.468282   \n",
       "2  0.000000   0.000000  0.871247  0.490845  0.000000  0.000000  0.000000   \n",
       "3  0.614189   0.000000  0.000000  0.000000  0.614189  0.000000  0.495524   \n",
       "4  0.000000   0.000000  0.000000  0.370086  0.000000  0.000000  0.000000   \n",
       "\n",
       "     sale     sun   today  weather  \n",
       "0  0.0000  0.6569  0.0000   0.6569  \n",
       "1  0.0000  0.0000  0.0000   0.0000  \n",
       "2  0.0000  0.0000  0.0000   0.0000  \n",
       "3  0.0000  0.0000  0.0000   0.0000  \n",
       "4  0.6569  0.0000  0.6569   0.0000  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['The weather is hot under the sun',\n",
    "          'I make my hot chocolate with milk',\n",
    "          'One hot encoding',\n",
    "          'I will have a chai latte with milk',\n",
    "          'There is a hot sale today']\n",
    "\n",
    "# Create the document-term matrix with TF-IDF vectorizer\n",
    "cv_tfidf = TfidfVectorizer(stop_words = \"english\")\n",
    "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
    "dt_tfidf = pd.DataFrame(X_tfidf, columns = cv_tfidf.get_feature_names_out())\n",
    "dt_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "0K4-aRyIk03P",
    "outputId": "98b5cf38-511b-4b8d-bb11-973afa5aa989"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[0.23204486]]),\n",
       "  ('I make my hot chocolate with milk', 'I will have a chai latte with milk')),\n",
       " (array([[0.18165505]]),\n",
       "  ('The weather is hot under the sun', 'One hot encoding')),\n",
       " (array([[0.18165505]]), ('One hot encoding', 'There is a hot sale today')),\n",
       " (array([[0.16050661]]),\n",
       "  ('I make my hot chocolate with milk', 'One hot encoding')),\n",
       " (array([[0.1369638]]),\n",
       "  ('The weather is hot under the sun', 'There is a hot sale today')),\n",
       " (array([[0.12101835]]),\n",
       "  ('The weather is hot under the sun', 'I make my hot chocolate with milk')),\n",
       " (array([[0.12101835]]),\n",
       "  ('I make my hot chocolate with milk', 'There is a hot sale today')),\n",
       " (array([[0.]]),\n",
       "  ('The weather is hot under the sun', 'I will have a chai latte with milk')),\n",
       " (array([[0.]]), ('One hot encoding', 'I will have a chai latte with milk')),\n",
       " (array([[0.]]),\n",
       "  ('I will have a chai latte with milk', 'There is a hot sale today'))]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
    "results_tfidf = [cosine_similarity([X_tfidf[a_index]], [X_tfidf[b_index]]) for (a_index, b_index) in pairs]\n",
    "sorted(zip(results_tfidf, combos), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Which Two Documents are Most Similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abfHLHtzk03P"
   },
   "source": [
    "![](https://i.imgur.com/mj4J60v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_344udsk03P"
   },
   "source": [
    "## Text Similarity Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04ubfzrMk03P"
   },
   "source": [
    "We will be using a song lyric dataset from Kaggle to identify songs with similar lyrics. The data set contains artists, songs and lyrics for 55K+ songs, but today we will be focusing on songs by one group in particular - The Beatles. The following code will help you load in the data and get set up for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwTzQeppk03Q"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5F7qQISVk03Q"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('songdata.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fx7fvF-mk03Q"
   },
   "source": [
    "### Question 1: Note the '\\n' (new line) characters in the lyrics. Remove them using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KL66xQSJk03Q"
   },
   "outputs": [],
   "source": [
    "# Code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6UNCrJUk03Q"
   },
   "source": [
    "### Question 2: List all the rows with \"Imagine\" in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRZO0iWyk03Q"
   },
   "outputs": [],
   "source": [
    "# Code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JYSXoK5k03R"
   },
   "source": [
    "### Question 3: Extract the first line of lyric out from the first song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Trrywo8Xk03R"
   },
   "outputs": [],
   "source": [
    "# Code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5QT9hM8k03R"
   },
   "source": [
    "### Question 4: Find out the sentiment of the extracted lyric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1m6qTGrPk03R"
   },
   "outputs": [],
   "source": [
    "# Code?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "KY_NLP(S) - Student.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
