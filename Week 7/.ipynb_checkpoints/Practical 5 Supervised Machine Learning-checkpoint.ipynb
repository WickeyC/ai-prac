{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section A: An Intro to Machine Learning and NLP\n",
    "\n",
    "<img src=\"pic7.png\" width=\"700\">\n",
    "\n",
    "https://www.javatpoint.com/machine-learning\n",
    "\n",
    "## Supervised Machine Learning\n",
    "\n",
    "<img src=\"pic1.png\" width=\"800\">\n",
    "\n",
    "https://karthikvegeta.medium.com/welcome-to-the-hood-of-machine-learning-199dd31f39e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification vs. Regression: Weather Forecasting\n",
    "\n",
    "<img src=\"pic2.jpeg\" width=\"700\">\n",
    "\n",
    "https://medium.com/@ali_88273/regression-vs-classification-87c224350d69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "▪ Logistic Regression\n",
    "\n",
    "▪ Naive Bayes\n",
    "\n",
    "▪ Comparing Methods: Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does Logistic Regression Work?\n",
    "\n",
    "▪ A binary classification task: To predict whether an animal is a cat or not.\n",
    "\n",
    "<img src=\"pic8.png\" width=\"600\">\n",
    "\n",
    "https://towardsdatascience.com/analytics-building-blocks-binary-classification-d205890314fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i163.photobucket.com/albums/t281/kyin_album/m6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fruits Classifier?\n",
    "\n",
    "▪ A multiclass classification task: To predict whether a fruit is an apple, orange or grapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i163.photobucket.com/albums/t281/kyin_album/m1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section B: Classification with Logistic Regression\n",
    "\n",
    "## Classification Task: Detection of spam emails\n",
    "\n",
    "<img src=\"pic4.png\" width=\"500\">\n",
    "\n",
    "https://developers.google.com/machine-learning/guides/text-classification\n",
    "\n",
    "## Step 1: Preparing Data \n",
    "\n",
    "▪ Make sure the data in **spam.txt** is labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_table('spam.txt', encoding = 'windows-1252', header = None)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding New Labels to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['label', 'text']\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm Up Exercise\n",
    "\n",
    "▪ **Question**: Swapping the \"label\" and \"text\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = data.reindex(columns = ['text', 'label'])\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk-through Examples Before Data Preprocessing\n",
    "\n",
    "### Example 1A: Normal Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a normal function that identifies an even number\n",
    "def even(num):\n",
    "    return num % 2 == 0\n",
    "\n",
    "nums = [5, 10]\n",
    "\n",
    "print(nums[0], \"is even:\", even(nums[0]))\n",
    "print(nums[1], \"is even:\", even(nums[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(even))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Lambda\n",
    "\n",
    "▪ Python lambda functions are **small, anonymous functions** defined using the **lambda** keyword.\n",
    "\n",
    "<img src=\"pic9.png\" width=\"450\">\n",
    "\n",
    "https://www.scaler.com/topics/how-to-use-lambda-functions-in-python/\n",
    "\n",
    "### Example 1B: Lambda Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lambda function that identifies an even number\n",
    "result = lambda num: num % 2 == 0\n",
    "\n",
    "nums = [5, 10]\n",
    "\n",
    "print(nums[0], \"is even:\", result(nums[0]))\n",
    "print(nums[1], \"is even:\", result(nums[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm Up Exercise\n",
    "\n",
    "▪ **Question**: Create and use a lambda function that multiplies two numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking Functions with and without Parentheses\n",
    "\n",
    "▪ When we call a function with parentheses, the function gets execute and returns the result to the callable.\n",
    "\n",
    "▪ When we call a function without parentheses, a function reference is sent to the callable rather than executing the function itself.\n",
    "\n",
    "https://www.geeksforgeeks.org/python-invoking-functions-with-and-without-parentheses/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_A(num = 5):\n",
    "    print(\"Function A\")\n",
    "\n",
    "def func_B(num = 10):\n",
    "    print(\"Function B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_B(func_A())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_B(func_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"I love AVENGER movies, especially The Endgame.\"\n",
    "\n",
    "def lowercase(match_obj):\n",
    "    return match_obj.group(0).lower()\n",
    "\n",
    "clean_text = re.sub(r\"\\b[A-Z]+\\b\", lowercase, text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Series map()\n",
    "\n",
    "▪ The map() function is used for substituting each value in a Series with another value, that may be derived from a function, a dict or a Series.\n",
    "\n",
    "https://www.w3resource.com/pandas/series/series-map.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocessing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before preprocessing data... \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Remove numbers\n",
    "x_number = lambda x: re.sub(r\"\\w*\\d\\w*\", '', x)\n",
    "\n",
    "# Remove punctuation and capital letters\n",
    "x_punc_upper = lambda x: re.sub('[%s]' %(string.punctuation), '', x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.text.map(x_number).map(x_punc_upper)\n",
    "\n",
    "# After preprocessing data...\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Splitting Data into Input and Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▪ **Input**: Features, Predictors, Independent Variables, X's \n",
    "    \n",
    "▪ **Output**: Label, Outcome, Dependent Variable, Y\n",
    "    \n",
    "![](https://i163.photobucket.com/albums/t281/kyin_album/m2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs to be fed into the model\n",
    "X = data.text\n",
    "\n",
    "# Output of the model\n",
    "y = data.label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "▪ Overfitting refers to a model that models the training data too well.\n",
    "\n",
    "![](pic5.png)\n",
    "\n",
    "https://www.ibm.com/cloud/learn/overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Avoid Overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i163.photobucket.com/albums/t281/kyin_album/m4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Splitting Data into Training Data and Test Data\n",
    "\n",
    "### train_test_split()\n",
    "\n",
    "▪ X: independent variable(s)\n",
    "\n",
    "▪ y: dependent variable\n",
    "\n",
    "▪ test size = 30% of observations, which means training size = 70% of observations\n",
    "\n",
    "▪ random state = 42, so we all get the same random train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pic6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Numerically Encoding the Input Data\n",
    "\n",
    "![](pic10.png)\n",
    "\n",
    "https://www.educative.io/answers/countvectorizer-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train).toarray()\n",
    "\n",
    "print(X_train_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data using the same vocabularies\n",
    "X_test_cv = cv.transform(X_test).toarray() \n",
    "\n",
    "print(X_test_cv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Fitting The Model and Predicting Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "lr.fit(X_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the model that was trained on the X_train_cv data and apply it to the X_test_cv\n",
    "y_pred_cv = lr.predict(X_test_cv)\n",
    "\n",
    "# The output is all of the predictions/ labels\n",
    "y_pred_cv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluating The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i163.photobucket.com/albums/t281/kyin_album/m5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_cv)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.heatmap(cm, xticklabels=['predicted_ham', 'predicted_spam'], yticklabels=['actual_ham', 'actual_spam'], \n",
    "            annot=True, fmt='d', annot_kws={'fontsize':20}, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_neg, false_pos = cm[0]\n",
    "false_neg, true_pos = cm[1]\n",
    "\n",
    "accuracy = round((true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg), 3)\n",
    "precision = round((true_pos) / (true_pos + false_pos), 3)\n",
    "recall = round((true_pos) / (true_pos + false_neg), 3)\n",
    "f1 = round(2 * (precision * recall) / (precision + recall), 3)\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 Score: {}'.format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section C: Classification with Naive Bayes\n",
    "\n",
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create a Naive Bayes prediction model object\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "nb.fit(X_train_cv, y_train)\n",
    "\n",
    "# Take the model that was trained on the X_train_cv data and apply it to the X_test_cv\n",
    "y_pred_cv_nb = nb.predict(X_test_cv)\n",
    "\n",
    "# The output is all of the predictions\n",
    "y_pred_cv_nb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_cv_nb)\n",
    "\n",
    "sns.heatmap(cm, xticklabels=['predicted_ham', 'predicted_spam'], yticklabels=['actual_ham', 'actual_spam'],\n",
    "annot=True, fmt='d', annot_kws={'fontsize':20}, cmap=\"YlGnBu\")\n",
    "\n",
    "true_neg, false_pos = cm[0]\n",
    "false_neg, true_pos = cm[1]\n",
    "\n",
    "accuracy = round((true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg), 3)\n",
    "precision = round((true_pos) / (true_pos + false_pos), 3)\n",
    "recall = round((true_pos) / (true_pos + false_neg), 3)\n",
    "f1 = round(2 * (precision * recall) / (precision + recall), 3)\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 Score: {}'.format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section D: NLP Showcase\n",
    "\n",
    "## Name Gender Classifier\n",
    "\n",
    "▪ To create a classifier that would automatically classify a given name into either male or female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "\n",
    "male_names = names.words(\"male.txt\")\n",
    "female_names = names.words(\"female.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list = [(name, 'male') for name in male_names]\n",
    "names_list += [(name, 'female') for name in female_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gender_features(name):\n",
    "    \n",
    "    # Convert all names to lowercase\n",
    "    name = name.lower()\n",
    "    \n",
    "    # Create an empty dictionary\n",
    "    features = {}\n",
    "    \n",
    "    # Extract different lengths of suffixes from names as features\n",
    "    features[\"suffix1\"] = name[-1:]\n",
    "    features[\"suffix2\"] = name[-2:] if len(name) > 1 else name[0]\n",
    "    features[\"suffix3\"] = name[-3:] if len(name) > 2 else name[0]\n",
    "    #features[\"suffix4\"] = name[-4:] if len(name) > 3 else name[0]\n",
    "    #features[\"suffix5\"] = name[-5:] if len(name) > 4 else name[0]\n",
    "    #features[\"suffix6\"] = name[-6:] if len(name) > 5 else name[0]\n",
    "    \n",
    "    # Extract different lengths of prefixes from names as features\n",
    "    features[\"prefix1\"] = name[:1]\n",
    "    features[\"prefix2\"] = name[:2] if len(name) > 1 else name[0]\n",
    "    features[\"prefix3\"] = name[:3] if len(name) > 2 else name[0]\n",
    "    #features[\"prefix4\"] = name[:4] if len(name) > 3 else name[0]\n",
    "    #features[\"prefix5\"] = name[:5] if len(name) > 4 else name[0]\n",
    "    #features[\"wordLen\"] = len(name)\n",
    "   \n",
    "    return features\n",
    "\n",
    "data = [(extract_gender_features(name), gender) for (name, gender) in names_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a limit for splitting training and testing data\n",
    "train_count = int(.8 * len(data))\n",
    "train_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the first 80% (the value of trainCount) dataset as the training data\n",
    "train_data = data[:train_count]\n",
    "\n",
    "# Make the remaining dataset as the test data\n",
    "test_data = data[train_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "bayes = nltk.NaiveBayesClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the classifier\n",
    "# Code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use classify() to do gender prediction\n",
    "prediction = [(bayes.classify(features), bayes.classify(features) == label) for features, label in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_test = names_list[train_count:]\n",
    "\n",
    "# Create an empty list to store name, gender, prediction, true/false\n",
    "result = []\n",
    "\n",
    "# Use sum() to combine two tuples into a new tuple\n",
    "for index in range(len(prediction)):\n",
    "    result.append(sum((names_test[index], prediction[index]), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(result, columns = ['Name', 'Gender', 'Prediction', 'T/F'])\n",
    "df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance in terms of accuracy\n",
    "print(\"Test data accuracy =\", nltk.classify.accuracy(bayes, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the 25 most informative features that our model used\n",
    "bayes.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all incorrect predictions\n",
    "errors = []\n",
    "\n",
    "for (name, label) in names_list:\n",
    "    if bayes.classify(extract_gender_features(name)) != label:\n",
    "        errors.append({\"name\": name, \"label\": label})\n",
    "\n",
    "errors[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section E: Machine Learning and NLP Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### Instructions\n",
    "\n",
    "▪ We will be using a review dataset from Kaggle (e.g., coffee.csv) for this exercise. \n",
    "\n",
    "▪ The product we'll be focusing on this time is a cappuccino cup.\n",
    "\n",
    "▪ Later on, split your dataset based on the ratio of 80% training data + 20% testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Preparing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Preprocessing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Splitting Data into Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Splitting Data into Training Data and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Numerically Encoding the Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Fitting Different Models and Predicting Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  II. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Evaluating Predictive Models\n",
    "\n",
    "**Example Output**:\n",
    "\n",
    "Accuracy score for LR  = 0.1651<br>\n",
    "Accuracy score for NB  = 0.6514<br>\n",
    "Accuracy score for SVM = 0.5413<br>\n",
    "Accuracy score for DT  = 0.5505<br>\n",
    "Accuracy score for RF  = 0.5872<br>\n",
    "Accuracy score for KNN = 0.5963<br>\n",
    "Accuracy score for NB  = 0.6514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Predict the rate of this review: **\"I dislike this coffee, terrible taste and very greasy.\"** by using Linear Regression, SVM, Decision Tree, Random Forest, KNN, and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
